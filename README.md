We're a team that set out to build a local-first consumer AI app across Apple, Windows and Android, but we 6 motnhs in we realized the hardware and software aren't there yet. Running near-realtime workloads on consumer CPUs and GPUs isn't feasible yet, it's too slow and drains battery life.

While some solutions exist for running local AI models on AI accelerators, most are closed source or only partially open, which we found frustrating. Rather than wait for others to solve this problem, we decided to tackle it ourselves and share our models and SDKs with everyone. 

Join our [discord](https://discord.gg/8FbwRaDFJR) or visit our [Github](https://github.com/FluidInference)