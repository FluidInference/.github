We're a team that was trying to build a local-first consumer AI app but quickly came to the realization that the hardware and software haven't caught up yet. Running near-realtime workloads on CPU/GPU for a local AI app on consumer devices just isn't feasible. It's slow and it drains your battery. There are some solutions out there for local AI models on AI accelerators, but most were closed source or partially open, and that was frustrating. 

We decided to get into this ourselves and share the models + SDKs. We believe that most inference will be run locally in the near future and want to accelerate that by committing to be fully open source.

Join our [discord](https://discord.gg/8FbwRaDFJR) or visit our [Github](https://github.com/FluidInference)